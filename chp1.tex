\chapter{Introduction}

The field of neuroscience is relatively new. Unlike other subjects that
have been studied for centuries, the exploration of the brain began less
than two centuries ago \cite{Ghosh}. In fact, the brain
remains shrouded in mystery, particularly regarding the information
processing mechanisms. Modelling and studying the brain can aid in
unravelling these mysteries and offer insights into neural architecture and
information processing. Comprehending these mechanisms not only enables the
harnessing of their potential for the creation of innovative bio-mimetic
technologies, but also aids in gaining a deeper understanding through
modelling.\\

In the late 1960s, it was clear that certain limitations 
would impede the progress of electronic device technology 
\cite{Hoeneisen}, and Moore's Law \cite{Moore}
was anticipated to reach its limits. Unfortunately, 
standard integrated circuit fabrication scaling processes 
are approaching fundamental physical constraints \cite{Shalf}. 
Thus, there is the need for an entirely new class of devices free from 
these limitations \cite{Mead1990}. One promising avenue involves 
exploring brain-inspired computing architectures with 
the aim of replicating the flexibility and computational 
efficiency observed in biological neural processing systems 
\cite{Frenkel2023}.\\
Moreover, the costs and energy consumption of \acrfull{ai} technologies are escalating exponentially \cite{DeVries}.
Currently, \acrshort{ai} technologies are characterised by an inefficient 
structure that demands significant power. This form of 
machine learning with a loosely brain-inspired architecture, 
might benefit from a shift towards a neuromorphic structure to 
address the power consumption issue \cite{Mead2023}.\\

The term \enquote{neuromorphic} was coined by Caltech Professor
Carver Mead, who observed that the MOS transistor operating 
in the subthreshold regime could easily replicate the dynamics 
of brain ion channels \cite{Mead1989}. He was convinced that \enquote{we have something fundamental to learn from 
the brain about a new and much more effective form of computation. 
Even the simplest brains of the simplest animals are awesome 
computational instruments. They do computations we do not know 
how to do, in ways we do not understand} \cite{Mead1990}.\\
For example, although the honeybee's brain consists of about 
one million neurons with a power budget of only 10 $\mu W$, it can 
perform real-time navigation and complex pattern recognition,
adapting to an environment in continuous
evolution \cite{Sandin}.\\

Neuromorphic engineering uses notions from the fields of neuroscience,
electrical engineering, and informatics to study biomimetic systems. From a
biological point of view, vision takes place when the retina converts 
photons into spikes. The smell is just volatilised molecules converted into
spikes, like the sense of touch is the conversion of tactile pressure. All
sensory inputs are thus encoded into spikes. This allows for event-driven
processing and low-precision computation.\\
Another attribute of biological neural processing systems includes the 
tight collocation of computation and memory. Therefore, the aim of 
neuromorphic technologies is the realisation of two paramount innovations 
in their systems. Contrary to traditional von Neumann processor 
architectures (computers) that separate processing (\acrfull{cpu}) and memory (\acrshort{ram}), 
the brainâ€™s organising principles are based on distributed computation, 
aligning processing and memory with neurons and synapses 
\cite{IndiveriLiu}. This new disposition has the potential to lower wiring, 
data transfer costs and energy supplied. 
Secondly, conventional processors encode data as multi-bit words 
processed sequentially under a global clock. Time is thus a 
by-product of computation. Conversely, the brain processes 
information by encoding data both in space and time with 
binary (all-or-none) events, where \enquote{time represents itself}.\\

Adopting a similar approach is advantageous for neuromorphic hardware for 
several reasons. A spike can be effectively approximated using a single 
bit (0 or 1), simplifying hardware representation compared to high-precision 
values. Another significant benefit is the inherent sparsity of 
neural activity; neurons are mostly silent. Leveraging this property proves
cost-effective, as storing 0 values requires fewer resources. Additionally,
the event-driven processing paradigm involves handling information only
when there is new data, avoiding the need for processing at every time 
step. This not only enhances power efficiency but also results in faster 
processing speeds, enabling operation in biological or even accelerated 
time.\\

The building block for realising this paradigm shift
is thus the \acrfull{snn}, that will be
soon described.
It is also called the third generation of neural network \cite{MAASS}.\\
In fact, neural networks have a long history, started in 1943 \cite{MCCULLOCH1990}. 
The term neural network is referred to graphs with a hierarchical
structure of the nodes that resemble the cortical layers organization. 
Each node receives a series of inputs that are first integrated
and then fed to a function that computes an output.\\
The first generation of neural network is the McCulloch-Pitts Neuron \cite{McCulloch}.
In this model, the binary inputs are added and then passed to a function that outputs a binary value depending on whether the result of the sum is greater than a certain threshold. 
This model can accept both excitatory and inhibitory (negative) inputs, and it was mainly used to model logic gates. 
Figure \ref{fig:MP neuron} shows a 3-inputs AND gate.\\

\begin{figure}[hbt!]
\centering
\begin{circuitikz}
    \draw (2.3,0.25) -- (0,1) node[left] {$x_{0}$};
    \draw (2.25,0) -- (0,0) node[left] {$x_{1}$};
    \draw (2.3,-0.25) -- (0,-1) node[left] {$x_{2}$};
    \draw (-0.05,0) node[circle, draw, inner sep=1pt]{};
    \draw (-0.05,1) node[circle, draw, inner sep=1pt]{};
    \draw (-0.05,-1) node[circle, draw, inner sep=1pt]{};
    \draw (3,0) circle (0.75);
    \draw (3,0) node {$f$};

    \draw (6.05,0) node[circle, fill=black, draw, inner sep=1pt]{};
    \draw (3.75,0) -- (6,0) node[right] {$y$};
\end{circuitikz}\\

\centering
$if\ x_{0}+x_{1}+x_{2}\geq3,\ y=1$
\caption[McCulloch-Pitts Neuron]{Description of a three inputs AND gate implemented using the McCulloch-Pitts model. All the inputs and the outputs are binary (0 or 1). The output is 1 only when the sum of the inputs is equal to 3. Otherwise, the output is 0.}
\label{fig:MP neuron}
\end{figure}

The second generation \cite{Nair} marked the advent of \acrfull{dnn}, featuring the \acrfull{an} (Figure \ref{fig:ANN}) as its 
fundamental component. Diverging from the McCulloch-Pitts model, these 
networks comprise new layers different from the input and output ones, 
called hidden layers. The \acrshort{an} comprises weighted continuous inputs that are 
aggregated and processed through an activation function, typically 
nonlinear. The resulting output from this function is continuous.\\
An essential aspect of these models lies in the weights associated with 
each node in the network. Among various methods for adjusting these weights 
(training the network) to make the network able to perform a particular 
task, the widely employed algorithm is backpropagation. Exploiting this 
model has led to diverse network types, such as \acrfull{cnn}, used for image recognition applications like 
Google Lens, and \acrfull{rnn}, applied in fields such as speech 
recognition and language modelling, exemplified by systems like ChatGPT and 
Gemini.\\

\begin{figure}[hbt!]
    \centering
    \begin{circuitikz}
        \draw (2.3,0.25) -- (1.3,1);
        \draw (2.25,0) -- (1.3,0);
        \draw (2.3,-0.25) -- (1.3,-1);
        \draw (1,1) node[circle, draw, inner sep=1pt] {$w_{0}$};
        \draw (1,0) node[circle, draw, inner sep=1pt] {$w_{1}$};
        \draw (1,-1) node[circle, draw, inner sep=1pt] {$w_{2}$};
        \draw (0.7,1) -- (-0.5,1) node[left] {$x_{0}$};
        \draw (0.7,-1) -- (-0.5,-1) node[left] {$x_{2}$};
        \draw (0.7,0) -- (-0.5,0) node[left] {$x_{1}$};

        \draw (-0.55,0) node[circle, draw, inner sep=1pt]{};
        \draw (-0.55,1) node[circle, draw, inner sep=1pt]{};
        \draw (-0.55,-1) node[circle, draw, inner sep=1pt]{};
        \draw (3,0) circle (0.75);
        \draw (3,0) node {$\sum$};
        \draw (5,0) node {$a$};
        \draw (5,0) circle (0.6);
    
        \draw (7.05,0) node[circle, fill=black, draw, inner sep=1pt]{};
        \draw (5.6,0) -- (7,0) node[right] {$y$};
        \draw (3.75,0) -- (4.4,0);

        \draw (3,0.75) -- (3,2);
        % \draw (3,2.25) rectangle (0.2,0.2);
        \draw (3,2.2) node {$b$};
    \end{circuitikz}\\
    
    \centering
    $y=a\left(\sum_{i} w_ix_i+b\right)$
    \caption[Artificial Neuron]{An artificial neuron has continuous inputs and outputs. 
    Each input is associated with a weight that determines the relevance of the input 
    in the computation. The weighted inputs are summed together 
    with a bias ($\sum$) and the result feed the activation 
    function ($a$) that determines the output.}
    \label{fig:ANN}
    \end{figure}

In the end, the \acrfull{sn} (Figure \ref{fig:SNN})
is the building block of the third generation of neural networks. Here, 
every neuron receives different synaptic binary inputs, and it outputs 
spikes based on its internal dynamic. This dynamic can be modelled in 
different ways and with different levels of detail and will be described in
Chapter \ref{ch3}.
The response of an \acrshort{sn} depends on its past inputs, while an \acrshort{an} 
responds only to its instantaneous inputs. This key feature allows 
for the temporal encoding of information.\\

\begin{figure}[ht]
    \centering
    \begin{circuitikz}
        \draw (2.3,0.25) -- (1.3,1);
        \draw (2.25,0) -- (1.3,0);
        \draw (2.3,-0.25) -- (1.3,-1);
        \draw (1,1) node[circle, draw, inner sep=1pt] {$w_{0}$};
        \draw (1,0) node[circle, draw, inner sep=1pt] {$w_{1}$};
        \draw (1,-1) node[circle, draw, inner sep=1pt] {$w_{2}$};
        \draw (0.7,1) -- (-3,1) node[left] {$x_{0}$};
        \draw (0.7,-1) -- (-3,-1) node[left] {$x_{2}$};
        \draw (0.7,0) -- (-3,0) node[left] {$x_{1}$};

        \draw (3,0) circle (0.75);
        \draw (3,0) node {$D$};
        \draw (3.75,0) -- (7,0) node[right] {$y$};

        \draw (-2.8,1) -- (-2.8,1.3);
        \draw (-2.7,-1) -- (-2.7,-0.7);
        \draw (-2.5,0) -- (-2.5,0.3);
        \draw (-2,-1)   -- (-2,-0.7);
        \draw (-1.9,1) -- (-1.9,1.3);
        % \draw (-1.7,0) -- (-1.7,0.3);
        \draw (-1.3,-1) -- (-1.3,0-0.7);
        \draw (-0.9,0) -- (-0.9,0.3);
        \draw (-0.5,-1) -- (-0.5,-0.7);
        \draw (-0.4,1) -- (-0.4,1.3);
        \draw (-0.1,0)    -- (-0.1,0.3);
        
        \draw (4.3,0) -- (4.3,0.3);
        \draw (6.2,0) -- (6.2,0.3);

    \end{circuitikz}\\
    \caption[Spiking Neuron]{The spiking neuron consists in a series of binary inputs (spikes) each one associated with a weight. These synaptic inputs alter the neural dynamic ($D$) that produces a binary output. The neural dynamic is a set of continuous equations that simulate the behaviour of the membrane potentialm of a neuron. The detailed discussion on this topic is delayed to Chapter} %\ref{ch3}.}
    \label{fig:SNN}
    \end{figure}

The PQN model \cite{Nanami} is one way of modelling the internal dynamics 
of the neurons using a qualitative approach. It ensures higher biological 
accuracy while maintaining more compact and lower power consumption than 
more bio-plausible alternatives, like conductance-based models. Remarkably 
versatile, the PQN model can represent diverse neuron classes and 
prioritises efficient implementation through digital arithmetic circuits. 
These characteristics make it a low-power tool for implementing \acrshort{sinn} on \acrshort{fpga} chips.\\
Indeed, the \acrshort{pqn} model aims to achieve exceptional \acrshort{ai} information processing performance with minimal power consumption, 
allowing it to mimic biological systems closely \cite{Levi2018}.\\

The applications of these devices are incredibly broad. A multitude of 
event-based sensors has already been developed. The event-based paradigm 
also applies to control. Indeed, various methods have been proposed to 
achieve robot locomotion.\\
Furthermore, the recent availability of extensive anatomical and 
physiological data has led to the development of data-driven SNN models 
\cite{Bezaire} 
with the ambitious goal of replicating the mammalian brain.\\
Increasing the level of biological accuracy, these models are thus suited 
for artificially recreating brain regions, also with a high level of detail.
Thanks to these attributes and the real-time simulation speed of \acrshort{sinn}, 
they can also play a crucial role in the treatment of neurological 
disorders.\\

Currently, one of the most significant challenges in neuroscience revolves around finding effective treatments for numerous chronic and incurable brain conditions. Worldwide, over one billion people have diseases and injuries affecting the brain, and existing treatments fail to address all symptoms while providing no guarantee of complete functional recovery. Among the myriad brain issues, stroke and \acrfull{tbi} stand out as they disrupt connectivity within the brain. Importantly, many of the neurochemical processes leading to cell death and disconnection also contribute to heightened susceptibility to plastic changes. Current therapeutic approaches attempt to leverage this plasticity through pharmacological treatments, which are still not highly effective, and robotic aids, which are both expensive and inadequately accessible \cite{Chiappalone}.
To advance plasticity recovery, innovative biological and engineering approaches geared towards brain rewiring are under exploration. This encompasses novel forms of neuroprosthetics \cite{GEORGE} capable of artificially reconnecting disconnected neuronal modules and substituting the activity of one module with real-time neuromorphic hardware \cite{Ambroise,BUCCELLI}.\\

The work of \cite{LEMASSON} established interactions between artificial and living neurons for the first time. Subsequently, the field witnessed the emergence of \enquote{hybrid neuromorphic engineering}, a concept that integrates technological devices with biological neurons \cite{Khoyratee}. This interdisciplinary approach enables real-time interaction between artificial and biological neurons \cite{Jung} facilitating learning adaptation, miniaturisation, and improved power consumption efficiency. SiNN-based neuroprostheses have demonstrated remarkable effectiveness in experiments focused on restoring locomotion in isolated spinal cords of rats subjected to complete transection \cite{Joucla}.\\

The objective of these new devices is to establish a closed-loop system \cite{LeviClosedLoop}, connecting the brain portion with neuromorphic hardware. Following the recording of neural activity with a \acrfull{mea}, a signal processing component endeavours to identify patterns within the recorded activity and, based on these patterns, deliver stimulation to the cells. This process can be executed in two ways, as illustrated in figure \ref{fig:closedloop}. A critical aspect of this procedure is the signal processing phase, during which the recorded signal from the \acrshort{mea} is analysed to extract spike events. Spike detection algorithms are typically employed for this task, but their lack of a universal method and absence of a definitive criterion pose challenges. In addressing these limitations, \acrshort{sinn} have been integrated into the signal processing stage instead of traditional spike detection algorithms. This choice is motivated by the \acrshort{sinn}s' capability to operate in real-time, implement bio-inspired processing techniques, and meet the growing demand for low-power and low-latency platforms. Notably, in the study by \cite{DOMINGUEZMORALES}, the SpiNNaker platform was utilised to implement a novel method for burst detection, yielding highly favourable results.\\

\begin{figure}[hbt!]
    \begin{center}
    \includegraphics[width=0.9\linewidth]{img/BNN-SNN.jpg}
    \end{center}
    \caption[Biohybrid System]{Figure taken from \cite{BUCCELLI}. (Top) The closed-loop system is depicted, wherein a BNN on a MEA and a SNN on a FPGA interact. The system initiates by recording activity from the biological population and extracting spikes through an event detection procedure. These extracted spikes then serve as input to the SNN, and the resulting output from the FPGA is transmitted back to the BNN. (Bottom) Two potential applications of this protocol are illustrated. In the first scenario, the SNN receives input spikes from the initial biological population and generates output spikes on a second population. In the second case, the SNN interfaces directly with a single population.}
    \label{fig:closedloop}
\end{figure}

The primary aim of this thesis is to extend the functionalities 
of the PQN model through the development of two new hardware 
designs to progress toward the realisation 
of an artificial thalamocortical system in a single hardware.\\
The first design encompasses the four predominant 
electrophysiological classes of cortical and thalamic 
neurons â€” namely, \acrfull{fs}, \acrfull{rs}, 
\acrfull{ib}, and \acrfull{lts} cells â€” integrated into a single chip.\\
In constructing 
thalamocortical networks, it is crucial to incorporate not 
only different classes of intrinsic properties but also the 
heterogeneity within each class 
\cite{Pospischil}. Notably, even neurons of the same 
type exhibit substantial variances in their properties. 
This variability contributes to the 
richness of neuronal processes, with the reliable 
functional behaviour of neural systems that depends critically 
on such diversity \cite{Lengler}. For this reason, 
the second design introduces cell-to-cell variability within 
each class, achieved by assigning specific parameters
to each individual neuron.\\

The thesis is structured as follows:\\
- Chapter \ref{ch2} provides a concise overview of digital logic systems, with a focus on programmable logic and FPGA.\\
- Chapter \ref{ch3} introduces the computational neuron models most used in neuromorphic hardware, specifically focusing on the PQN model.\\
- Chapter \ref{ch4} elucidates the characteristics of various neuromorphic hardware, emphasising the one proposed by \cite{Nanami}, which serves as the foundational design for this project.\\
- Chapter \ref{ch5} explains the two new hardware designs developed within this project.\\
- Chapter \ref{ch6} summarises applications and future perspectives.\\